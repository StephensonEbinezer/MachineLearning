{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL Pipeline for Client ComZ input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data imported\n",
      "Target Dataframe Created\n",
      "Indexing valid users\n",
      "Indexing valid users - Done\n",
      "Cleaning date column\n",
      "Cleaning date column - Done\n",
      "Case cleaning for the dataframe\n",
      "Case cleaning for the dataframe - Done\n",
      "Imputing date,Product and activity columns - Executing\n",
      "Imputing date,Product and activity columns - Done\n",
      "........Dataframe to previous 21 days - Executing..........\n",
      "........Dataframe to previous 21 days - Done..........\n",
      ".....Executing functions for feature exrtaction........\n",
      ".........Merging the data........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\steph\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:138: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "C:\\Users\\steph\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:146: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "C:\\Users\\steph\\Miniconda3\\lib\\site-packages\\ipykernel_launcher.py:158: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............Data merge done...............\n",
      ".............Exporting to csv - Executing........\n",
      ".............Exporting to csv - Done........\n"
     ]
    }
   ],
   "source": [
    "#Reading the data from CSV and creating target dataframe with unique user IDs\n",
    "Data_visit = pd.read_csv(r'C:\\Users\\steph\\Downloads\\data (1)\\data\\VisitorLogsData.csv')\n",
    "Data_user = pd.read_csv(r'C:\\Users\\steph\\Downloads\\data (1)\\data\\userTable.csv')\n",
    "print(\"Data imported\")\n",
    "\n",
    "# Creating the target dataframe with the unique values in the dataset\n",
    "Data_target = pd.DataFrame(sorted(Data_visit['UserID'].dropna().drop_duplicates()),columns=['UserID'])\n",
    "print(\"Target Dataframe Created\")\n",
    "\n",
    "#Picking the dataframe with valid users\n",
    "print(\"Indexing valid users\")\n",
    "Data_visit = Data_visit[Data_visit['UserID'].notnull()]\n",
    "print(\"Indexing valid users - Done\")\n",
    "\n",
    "# Defining the start date and end date for the dataframe\n",
    "Current_date = pd.to_datetime('2018-05-28') #System date can be passed in the future\n",
    "Current_date = datetime.date(Current_date)\n",
    "Start_date = pd.to_datetime(Current_date)-pd.to_timedelta(\"21day\")\n",
    "End_date = pd.to_datetime(Current_date)-timedelta(1)\n",
    "End_date = pd.to_datetime(datetime.combine(datetime.date(End_date),datetime.time(End_date).max))\n",
    "\n",
    "#Dropping duplicates\n",
    "Data_visit.drop_duplicates(inplace=True)\n",
    "\n",
    "# Function to clean the date columns\n",
    "def date_clean(Date_string):\n",
    "    if 'nan' in str(Date_string):\n",
    "        Date_string = np.nan\n",
    "#         Date_string = Date_string.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    elif \":\" not in str(Date_string):\n",
    "        Date_string = int(Date_string)\n",
    "        Date_string = datetime.fromtimestamp(Date_string//1000000000)\n",
    "        Date_string = Date_string.strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "    else:\n",
    "        Date_string = datetime.strftime(pd.to_datetime(Date_string),'%Y-%m-%d %H:%M:%S.%f')\n",
    "    return(Date_string)\n",
    "\n",
    "#Clean Date column\n",
    "print(\"Cleaning date column\")\n",
    "Data_visit['VisitDateTime'] = Data_visit['VisitDateTime'].apply(date_clean)\n",
    "print(\"Cleaning date column - Done\")\n",
    "\n",
    "# Converting the dataframe to upper case\n",
    "print(\"Case cleaning for the dataframe\")\n",
    "Data_visit = Data_visit.fillna('').astype(str).apply(lambda x: x.str.upper())\n",
    "Data_visit = Data_visit.replace('',np.nan)\n",
    "print(\"Case cleaning for the dataframe - Done\")\n",
    "\n",
    "\n",
    "print(\"Imputing date,Product and activity columns - Executing\")\n",
    "def median(x):\n",
    "    x = max(x) - ((max(x)-min(x))/2)\n",
    "    x = pd.to_datetime(x)\n",
    "    return(x)\n",
    "\n",
    "def clean_date(Dataframe,index,date_col,product_col,activity_col):\n",
    "    Dataframe[date_col] = pd.to_datetime(Dataframe[date_col])\n",
    "    Dataframe = Dataframe.sort_values([index,date_col])\n",
    "    Mask_1 = Dataframe[product_col].isna()\n",
    "    Fill_column_1 = Dataframe[product_col].ffill(limit=3)\n",
    "    Mask_11 = Fill_column_1.eq(Dataframe[product_col].bfill(limit=3))\n",
    "    Dataframe[product_col] = Dataframe[product_col].mask(Mask_1&Mask_11,Fill_column_1)\n",
    "    \n",
    "    Dataframe['Year_Month_Date']=Dataframe.groupby([index,product_col])[date_col].apply(lambda x:x.fillna(median(x)))\n",
    "    Dataframe[date_col] = Dataframe[date_col].transform(lambda x: x.fillna(Start_date))\n",
    "    Dataframe['Year_Month_Date']=Dataframe['Year_Month_Date'].transform(lambda x:x.fillna(Dataframe[date_col]))\n",
    "    \n",
    "    Dataframe = Dataframe.sort_values([index,'Year_Month_Date'])\n",
    "    Dataframe['Cumsum'] = (Dataframe[product_col]!=Dataframe[product_col].shift(1)).cumsum()\n",
    "    \n",
    "    Mask_2 = Dataframe[product_col].notnull()\n",
    "    Fill_column_2 = Dataframe.groupby([index,product_col,'Cumsum'])[activity_col].ffill()\n",
    "    Dataframe[activity_col] = Dataframe[activity_col].mask(Mask_2,Fill_column_2)\n",
    "    \n",
    "    Dataframe = Dataframe.sort_values([index,product_col,'Year_Month_Date'])\n",
    "    Dataframe['Cumsum'] = (Dataframe[product_col]!=Dataframe[product_col].shift(1)).cumsum()\n",
    "    \n",
    "    Mask_3 = Dataframe[product_col].notnull()\n",
    "    Fill_column_3 = Dataframe.groupby([index,product_col,'Cumsum'])[activity_col].ffill()\n",
    "    Dataframe[activity_col] = Dataframe[activity_col].mask(Mask_3,Fill_column_3)\n",
    "       \n",
    "    return(Dataframe)\n",
    "Data_visit = clean_date(Data_visit,'UserID','VisitDateTime','ProductID','Activity')\n",
    "print(\"Imputing date,Product and activity columns - Done\")\n",
    "\n",
    "\n",
    "print(\"........Dataframe to previous 21 days - Executing..........\")\n",
    "#Slicing the dataframe for previous 21 days\n",
    "Data_visit = Data_visit[Data_visit['Year_Month_Date'].between(Start_date,End_date)]\n",
    "print(\"........Dataframe to previous 21 days - Done..........\")\n",
    "\n",
    "\n",
    "print(\".....Executing functions for feature exrtaction........\")\n",
    "\n",
    "#Converting the date column to datetype\n",
    "Data_visit['Year_Month_Date'] = pd.to_datetime(Data_visit['Year_Month_Date'])\n",
    "\n",
    "def visits_7_days(Dataframe,index,date_col):\n",
    "    #getting the dataframe sliced to 7 days from the current date\n",
    "    Dataframe = Dataframe[Dataframe[date_col] >pd.to_datetime(Current_date)-pd.to_timedelta(\"7day\")]\n",
    "    #Getting the number of days visited in the last 7 days\n",
    "    Data = Dataframe.copy()\n",
    "    Data[date_col] = Data[date_col].apply(lambda x:datetime.date(pd.to_datetime(x)))\n",
    "    Data = Data.groupby(index).agg({date_col :\"nunique\"}).\\\n",
    "                rename(columns = {'Year_Month_Date':'No_of_days_Visited_7_Days'}).reset_index()\n",
    "    return(Data)\n",
    "\n",
    "\n",
    "def No_Of_Products_Viewed_15_Days(Dataframe,index,date_column,col_target):\n",
    "    #getting the dataframe sliced to 15 days from the current date\n",
    "    Dataframe = Dataframe[Dataframe[date_column] >pd.to_datetime(Current_date)-pd.to_timedelta(\"15day\")]\n",
    "    #Getting the number of days visited in the last 15 days\n",
    "    Data1 = Dataframe.groupby(index).agg({col_target:\"nunique\"}).\\\n",
    "                rename(columns = {'ProductID':'No_Of_Products_Viewed_15_Days'}).reset_index()\n",
    "    return(Data1)\n",
    "\n",
    "def User_Vintage(Dataframe,date_col,index):\n",
    "    #converting the string column of the dataframe to dateformat to perform the difference in days\n",
    "    Dataframe[date_col] = Dataframe[date_col].apply(lambda x:datetime.date(pd.to_datetime(x)))\n",
    "    Dataframe['User_Vintage'] = Current_date-Dataframe[date_col]\n",
    "    Dataframe['User_Vintage'] = Dataframe['User_Vintage'].apply(lambda x: x.days)\n",
    "    return(Dataframe[[index,'User_Vintage']])\n",
    "\n",
    "def Most_Viewed_product_15_Days(Dataframe,index,filter_col,target_col,date_col):\n",
    "    #Converting the date column to date format and slicing last 15 days\n",
    "    Dataframe = Dataframe[Dataframe[date_col] >pd.to_datetime(Current_date)-pd.to_timedelta(\"15day\")]\n",
    "    \n",
    "    #Preprocessing the dataframe for the conditions\n",
    "    Dataframe = Dataframe[Dataframe[filter_col] == 'PAGELOAD']\n",
    "    \n",
    "    #Grouping the data and selecting the most viewed product\n",
    "    temp = Dataframe.groupby([index,target_col]).agg({target_col:'count',date_col:'max'})\\\n",
    "            .rename(columns = {target_col:'Value_count'}).sort_values([index,'Value_count',date_col],ascending = False)\\\n",
    "            .reset_index()\n",
    "    Data_3 = temp.groupby(index)[index,target_col].head(1).reset_index(drop=True)\\\n",
    "                .rename(columns={target_col:'Most_Viewed_product_15_Days'})\n",
    "    return(Data_3)\n",
    "\n",
    "def Most_Active_OS(Dataframe,index,target_col,date_col):\n",
    "    temp = Dataframe.groupby([index,target_col]).agg({target_col:'count',date_col:'max'})\\\n",
    "            .rename(columns = {target_col:'Value_count'}).sort_values([index,'Value_count',date_col],ascending = False)\\\n",
    "            .reset_index()\n",
    "    Data_4 = temp.groupby(index)[index,target_col].head(1).reset_index(drop=True)\\\n",
    "                .rename(columns={target_col:'Most_Active_OS'})\n",
    "    return(Data_4)\n",
    "\n",
    "def Recently_Viewed_Product(Dataframe,index,filter_col,target_col,date_col):\n",
    "    #Preprocessing the dataframe for the conditions\n",
    "    Dataframe = Dataframe[Dataframe[filter_col] == 'PAGELOAD']\n",
    "    \n",
    "    #Grouping the data and selecting the most viewed product\n",
    "    temp = Dataframe.groupby([index,target_col]).agg({date_col:'max'})\\\n",
    "            .rename(columns = {target_col:'Value_count'}).sort_values([index,date_col],ascending = False)\\\n",
    "            .reset_index()\n",
    "    Data_5 = temp.groupby(index)[index,target_col].head(1).reset_index(drop=True)\\\n",
    "                .rename(columns={target_col:'Recently_Viewed_Product'})\n",
    "    return(Data_5)\n",
    "\n",
    "def Pageloads_last_7_days(Dataframe,index,date_col,filter_col):\n",
    "  \n",
    "    Dataframe = Dataframe[Dataframe[date_col] >pd.to_datetime(Current_date)-pd.to_timedelta(\"7day\")]\n",
    "    Dataframe = Dataframe[Dataframe[filter_col] == 'PAGELOAD']\n",
    "    Data_6 = Dataframe.groupby([index]).agg({index:'count'}).rename(columns={'UserID':'Pageloads_last_7_days'}).reset_index()\n",
    "    return(Data_6)\n",
    "\n",
    "def Clicks_last_7_days(Dataframe,index,date_col,filter_col):\n",
    "   \n",
    "    Dataframe = Dataframe[Dataframe[date_col] >pd.to_datetime(Current_date)-pd.to_timedelta(\"7day\")]\n",
    "    Dataframe = Dataframe[Dataframe[filter_col] == 'CLICK']\n",
    "    Data_7 = Dataframe.groupby([index]).agg({index:'count'}).rename(columns={'UserID':'Clicks_last_7_days'}).reset_index()\n",
    "    return(Data_7)\n",
    "\n",
    "\n",
    "\n",
    "print(\".........Merging the data........\")\n",
    "\n",
    "#Merging the visitors for last 7 days to the target dataframe\n",
    "Data_target = Data_target.merge(visits_7_days(Data_visit,'UserID','Year_Month_Date'),how='left',on='UserID')\n",
    "Data_target['No_of_days_Visited_7_Days'].fillna(0,inplace=True)\n",
    "Data_target['No_of_days_Visited_7_Days'] = Data_target['No_of_days_Visited_7_Days'].astype(int)\n",
    "\n",
    "#Merging the products viewed for last 15 days to the target dataframe\n",
    "Data_target = Data_target.merge(No_Of_Products_Viewed_15_Days(Data_visit,'UserID','Year_Month_Date','ProductID'),how='left',on='UserID')\n",
    "Data_target['No_Of_Products_Viewed_15_Days'].fillna(0,inplace=True)\n",
    "Data_target['No_Of_Products_Viewed_15_Days'] = Data_target['No_Of_Products_Viewed_15_Days'].astype(int)\n",
    "\n",
    "#merging the User_vintage to the target dataframe\n",
    "Data_target = Data_target.merge(User_Vintage(Data_user,'Signup Date','UserID'),how='left',on='UserID')\n",
    "Data_target['User_Vintage'].fillna(0,inplace=True)\n",
    "\n",
    "#merging the Most_Viewed_product_15_Days to the target dataframe\n",
    "Data_target = Data_target.merge(Most_Viewed_product_15_Days(Data_visit,'UserID','Activity','ProductID','Year_Month_Date')\\\n",
    "                                ,how='left',on='UserID')\n",
    "Data_target['Most_Viewed_product_15_Days'].fillna(\"Product101\",inplace=True)\n",
    "\n",
    "#merging the Most_Active_OS to the target dataframe\n",
    "Data_target = Data_target.merge(Most_Active_OS(Data_visit,'UserID','OS','Year_Month_Date'),how='left',on='UserID')\n",
    "# Data_target['Most_Active_OS'].fillna('NA',inplace=True)\n",
    "\n",
    "#merging the Recently_Viewed_Product to the target dataframe\n",
    "Data_target = Data_target.merge(Recently_Viewed_Product(Data_visit,'UserID','Activity','ProductID','Year_Month_Date')\\\n",
    "                                ,how='left',on='UserID')\n",
    "Data_target['Recently_Viewed_Product'].fillna(\"Product101\",inplace=True)\n",
    "\n",
    "#merging the Pageloads_last_7_days to the target dataframe\n",
    "Data_target = Data_target.merge(Pageloads_last_7_days(Data_visit,'UserID','Year_Month_Date','Activity')\\\n",
    "                                ,how='left',on='UserID')\n",
    "Data_target['Pageloads_last_7_days'].fillna(0,inplace=True)\n",
    "Data_target['Pageloads_last_7_days'] = Data_target['Pageloads_last_7_days'].astype(int)\n",
    "\n",
    "#merging the Clicks_last_7_days to the target dataframe\n",
    "Data_target = Data_target.merge(Clicks_last_7_days(Data_visit,'UserID','Year_Month_Date','Activity')\\\n",
    "                                ,how='left',on='UserID')\n",
    "Data_target['Clicks_last_7_days'].fillna(0,inplace=True)\n",
    "Data_target['Clicks_last_7_days'] = Data_target['Clicks_last_7_days'].astype(int)\n",
    "\n",
    "print(\"...............Data merge done...............\")\n",
    "\n",
    "\n",
    "print(\".............Exporting to csv - Executing........\")\n",
    "#Printing to CSV\n",
    "Data_target.to_csv('Jobathon_31.csv',index=False)\n",
    "print(\".............Exporting to csv - Done........\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
